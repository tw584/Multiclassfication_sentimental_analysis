{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eae8aa6c-0600-42e5-b0a1-cf03f02f85bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for the installation of pytorch\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b85b339-1a1d-471c-b216-0f5f48be4b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for the uninstallation of pytorch\n",
    "!pip uninstall torch -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeff5715-27e4-4ffc-bdda-a2bc1531bb3b",
   "metadata": {},
   "source": [
    "We encountered issues during the process of putting tensorflow and pytorch under the same environment (tensorflow is for the CNN model and pytorch is for the bert transformer). That is, if you want to run CNN with tensorflow, you first need to uninstall the pytorch package (vise versa). Also, we suggest run each model separately!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89c61e14-5ec5-4047-81a6-e21190422d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Packages for DistilBERT\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6143cb-902b-478a-b607-d649a0fcb438",
   "metadata": {},
   "source": [
    "<h3>1.1 Load and preprocess the dataset:</h3><p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b113590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset for both models\n",
    "train = pd.read_csv(\"/Users/jiaxuanliu/Downloads/cs-3780-5780-how-do-you-feel/train.csv\")\n",
    "train_text = train[\"text\"]\n",
    "train_label = train[\"label\"]\n",
    "\n",
    "test = pd.read_csv(\"/Users/jiaxuanliu/Downloads/cs-3780-5780-how-do-you-feel/test.csv\")\n",
    "test_id = test[\"id\"]\n",
    "test_text = test[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bd99ce-b859-424f-88d4-3ebdf50e8c5d",
   "metadata": {},
   "source": [
    "<h2>Model1: DistilBERT</h2><p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51612a5f-f746-4004-9a23-0b939c770a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 1:43:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.806200</td>\n",
       "      <td>0.936063</td>\n",
       "      <td>0.746500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.664300</td>\n",
       "      <td>0.824572</td>\n",
       "      <td>0.772000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.626600</td>\n",
       "      <td>0.814632</td>\n",
       "      <td>0.773000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 02:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results: {'eval_loss': 0.8146315813064575, 'eval_accuracy': 0.773, 'eval_runtime': 123.7404, 'eval_samples_per_second': 16.163, 'eval_steps_per_second': 1.01, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "\n",
    "# Model1 DistilBERT\n",
    "\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder() # Converts text labels to numerical format.\n",
    "train_label_encoded = label_encoder.fit_transform(train_label) # Encodes training labels.\n",
    "num_labels = len(label_encoder.classes_) # Total number of unique labels (classes)\n",
    "# 80% training and 20% validation split.\n",
    "\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_text_split, val_text_split, train_label_split, val_label_split = train_test_split(\n",
    "    train_text, train_label_encoded, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define Dataset class\n",
    "class EmotionDataset(Dataset): # Custom dataset for tokenization and formatting\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        # Input text data and labels\n",
    "        self.tokenizer = tokenizer # Pretrained tokenizer\n",
    "        self.max_len = max_len # Maximum tokenized sequence length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        label = self.labels[idx] if self.labels is not None else -1\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long) if label != -1 else torch.tensor(-1, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Initialize DistilBERT tokenizer and model\n",
    "model_name = \"distilbert-base-uncased\" # Pretrained DistilBERT model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name) # Load tokenizer\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "# Load DistilBERT model configured for multi-class classification\n",
    "\n",
    "# Create datasets\n",
    "max_len = 128\n",
    "train_dataset = EmotionDataset(train_text_split, train_label_split, tokenizer, max_len)\n",
    "val_dataset = EmotionDataset(val_text_split, val_label_split, tokenizer, max_len)\n",
    "test_dataset = EmotionDataset(test_text, None, tokenizer, max_len)\n",
    "\n",
    "# Define function to compute metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred # Extract predictions and true labels\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=-1) # Get predicted class\n",
    "    acc = accuracy_score(labels, predictions.numpy()) # Calculate accuracy\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\", # Save model checkpoints\n",
    "    eval_strategy=\"epoch\",  # Updated parameter name\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,  # Batch size for training\n",
    "    per_device_eval_batch_size=16, # Batch size for evaluation\n",
    "    num_train_epochs=3, # Train for 3 epochs\n",
    "    save_total_limit=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True, # Load the best model after training\n",
    "    metric_for_best_model=\"accuracy\", # Use accuracy to choose the best model\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model, # Model for training\n",
    "    args=training_args, # Training configurations\n",
    "    train_dataset=train_dataset, # Training dataset\n",
    "    eval_dataset=val_dataset, # Validation dataset\n",
    "    compute_metrics=compute_metrics # Function to compute evaluation metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "##\n",
    "\n",
    "# Evaluate the model on validation data\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Validation Results:\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d5ffc4-beff-4898-91aa-fe8a4101b8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./trained_model/tokenizer_config.json',\n",
       " './trained_model/special_tokens_map.json',\n",
       " './trained_model/vocab.txt',\n",
       " './trained_model/added_tokens.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained model and tokenizer\n",
    "model.save_pretrained(\"./trained_model\")\n",
    "tokenizer.save_pretrained(\"./trained_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb21230-207d-4aca-9a1d-4a51b24810c1",
   "metadata": {},
   "source": [
    "<h2>Model1: DistilBERT</h2><p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c40393a2-9f71-46bb-8207-9c9c375cdb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training & Validation & Prediction for DistilBERT\n",
    "# Define test dataset class\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        item = {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "\n",
    "        return item\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = EmotionDataset(test_text, labels=None, tokenizer=tokenizer, max_len=128)\n",
    "\n",
    "# Generate predictions\n",
    "from transformers import Trainer\n",
    "trainer = Trainer(model=model)\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# Decode predictions\n",
    "test_preds = torch.argmax(torch.tensor(predictions.predictions), dim=1)\n",
    "test_labels_decoded = label_encoder.inverse_transform(test_preds.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c6f1c56-aa32-4a72-99f7-1d7e86ea0797",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_BERT = pd.DataFrame({\"id\": test_id, \"label\": test_labels_decoded})\n",
    "submission_BERT.to_csv(\"submission_BERT.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e02474-b49b-4abc-8d29-847a07ea4229",
   "metadata": {},
   "source": [
    "<h2>Part 4: Resources and Literature Used</h2><p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed8be8a-3fa6-4f15-964a-b977abff7522",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/hub/transformers\n",
    "\n",
    "\n",
    "https://huggingface.co/docs/transformers/en/model_doc/distilbert"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
